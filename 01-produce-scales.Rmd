---
title: "Vehicles in the park"
output: 
  html_document: 
    code_folding: hide
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse) # for data manipulation
library(sirt) # used to fit the Bradley-Terry model
library(cocor) # used to compute correlation coefficients

source("cj_functions.R")

# Set preferred styling
theme_set(theme_minimal())
```

# Items being judged

The judgement data uses ID numbers for the various items being judged. This table shows how these IDs correspond to the actual items:

```{r message=FALSE, warning=FALSE}
items <- read_csv(here::here("data-raw/item_details.csv")) %>% mutate(item_description = str_trim(item_description))

items
```
# Judgement data

```{r message=FALSE}
judgement_data <- read_csv(here::here("data-raw", "judgements_all.csv"))

# restrict to the binary judgement data
# (some of the judges used a slider to provide a score rather than a binary choice)
judgement_data <- judgement_data %>% 
  filter(!is.na(won)) %>% 
  select(-any_of("score"))
```

Data summary:

```{r}
judgement_data %>% 
  group_by(study) %>% 
  summarise(
    n_judges = n_distinct(judge_id),
    n_judgements = n_distinct(judgement_id)
  )
```

Judges had to complete attention checks - when item 0 appeared, they were instructed to select that as the winner.

Here we identify those judges who failed any of the attention checks:

```{r}
judgement_data %>% 
  mutate(attention_check = case_when(
    (left == 0 | right == 0) & won == 0 ~ "pass",
    (left == 0 | right == 0) & won != 0 ~ "fail",
    TRUE ~ NA_character_
  )) %>% 
  group_by(judge_id) %>% 
  summarise(
    failed_attention_checks = sum(attention_check == "fail", na.rm = TRUE)
  ) %>% 
  filter(failed_attention_checks > 0)
```

For the analysis, we remove all the attention checks, and also all data from those judges who failed 2 or more of the attention checks.

```{r}
decisions <- judgement_data %>% 
  filter(left != 0, right != 0) %>% 
  filter(!judge_id %in% c(59, 110))
```

This leaves the following set of judgements:

```{r}
decisions %>% 
  group_by(study) %>% 
  summarise(
    n_judges = n_distinct(judge_id),
    n_judgements = n_distinct(judgement_id)
  )
```

# Fit the Bradley-Terry model

We fit the Bradley-Terry model separately for each study.

```{r include=FALSE}
btm_results <- decisions %>% 
  # group the judgements by study and fit the BT model
  nest(csv_content = !c(study)) %>% 
  mutate(
    btm_stuff = map(csv_content, btm_for_cj, chosen_col = "won", notchosen_col = "lost", judge_col = "judge_id")
  ) %>% 
  unnest(cols = c(btm_stuff))
```

```{r}
btm_results %>% 
  select(-csv_content, -btm_estimates, -judge_fits) %>% 
  mutate(ssr = as.numeric(ssr)) %>% 
  arrange(study)

btm_estimates <- btm_results %>% 
  select(study, btm_estimates) %>% 
  unnest(cols = c(btm_estimates)) %>% 
  select(-id) %>%
  # For clarity, include the item names corresponding to the ID numbers
  left_join(items %>% select(individual = item_num, item_name), by = "individual") %>% 
  relocate(item_name, .after = individual)
```

> **TODO** Split-halves reliability

The estimated scores for each item are written to `data-processed/btm_estimates.csv`.

```{r}
btm_estimates %>% 
  write_csv(here::here("data-processed/btm_estimates.csv"))
```

We also produce a simplified version that just includes the theta (_est) and se.theta (_se) for each of the scales, saved in `data-processed/scores.csv`.

```{r}
scores <- btm_estimates %>% 
  # remove redundant text from the study names
  mutate(study = str_remove(study, "_pairs")) %>% 
  # focus only on the relevant columns
  select(study, individual, item_name, est = theta, se = se.theta) %>% 
  # create separate _est and _se columns for each study
  pivot_wider(
    names_from = study,
    names_glue = "{study}_{.value}",
    values_from = c(est, se)
  )

scores

scores %>% 
  write_csv(here::here("data-processed/scores.csv"))
```


### Interlude: Judge fits

The model gives an infit figure for each judge. This code identifies judges who are more than 2 SDs above the mean.

```{r}
judge_fits <- btm_results %>% 
  select(study, judge_fits) %>% 
  unnest(cols = c(judge_fits))

judge_fits %>% 
  group_by(study) %>% 
  count(discard)

judge_fits %>% 
  filter(discard == TRUE)
```

This plot shows the infit score for each judge:

```{r}
judge_fits %>% 
  ggplot(aes(y = infit, x = judge_id)) +
  geom_point(alpha = 0.1) +
  geom_text(aes(label = judge_id)) +
  facet_grid(cols = vars(study))

```

Since very few judges are misfitting, we have decided not to remove their data.

### Interlude 2: "Every judge saw every item multiple times"

Here we check how many times each judge saw each item, and focus attention on the judge/item pairings that occurred the least:

```{r}
decisions %>% 
  select(judge_id, left, right) %>% 
  pivot_longer(c(left, right), values_to = "item") %>% 
  select(judge_id, item) %>% 
  group_by(judge_id, item) %>% 
  tally() %>% 
  ungroup() %>% 
  complete(judge_id, item) %>% 
  arrange(n) %>% 
  head(n = 20)
```

This shows that almost all judges saw every item at least twice. However it looks like judge 168 only saw some items once.


# Next: analysis

The analysis is carried out in a separate script, `02-analyse-scales`.
